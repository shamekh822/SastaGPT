{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0SqXxgq03fQ",
        "outputId": "d6ea70e4-dc33-43d5-a5de-a379f06f5226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Used for counting tokens \n",
        "!pip install tiktoken "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lvaOGBGR03fT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMSuOzB803fU",
        "outputId": "0a227432-007e-4314-e1da-c93c3beff975"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-bc1ffde2c3f9>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x=torch.tensor(x, dtype=torch.long)\n",
            "<ipython-input-7-bc1ffde2c3f9>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y=torch.tensor(y, dtype=torch.long)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(torch.Size([32, 8]), torch.Size([32, 8]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE=32\n",
        "BLOCK_SIZE=8\n",
        "\n",
        "encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "\n",
        "dataset=[]\n",
        "with open('/content/drive/MyDrive/interstellar_script.txt', \"r\", encoding='utf-8') as file:\n",
        "    dataset=file.read()\n",
        "\n",
        "# Creating the encoded ids using the encoder\n",
        "encoded_ids=encoder.encode(dataset)\n",
        "encoded_ids=torch.tensor(encoded_ids)\n",
        "\n",
        "\n",
        "def get_batch():\n",
        "    '''\n",
        "    Returns a batch (x, y) from the dataset\n",
        "    '''\n",
        "    ind=torch.randint(len(encoded_ids)-BLOCK_SIZE, (BATCH_SIZE, ))\n",
        "\n",
        "    x=torch.stack([encoded_ids[i:i+BLOCK_SIZE] for i in ind])\n",
        "    y=torch.stack([encoded_ids[i+1:i+BLOCK_SIZE+1] for i in ind])\n",
        "\n",
        "    x=torch.tensor(x, dtype=torch.long)\n",
        "    y=torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch()\n",
        "xb.shape, yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q36JhDJF1Y-a",
        "outputId": "a8d70476-07ba-4023-bedf-e77eb5a58045"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CbSxHmj03fV"
      },
      "source": [
        "## Creating the Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nNLBLfb803fV"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    block_size: int = BLOCK_SIZE\n",
        "    emb_dim: int = 256\n",
        "    head_size: int = 32\n",
        "    num_heads: int = 8\n",
        "    num_layers: int = 2\n",
        "    vocab_size: int = encoder.n_vocab # vocab size of the tokenizer\n",
        "\n",
        "# We like to have emb_dim == head_size * num_heads\n",
        "config = Config()\n",
        "assert config.emb_dim == config.head_size * config.num_heads, \"Embedding dimension must be divisible by number of heads\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnRQJ9R203fW"
      },
      "source": [
        "### Multi-Head Self-Attention\n",
        "\n",
        "This component is the core of the Transformer. This is where the model learns to attend to different parts of the input sequence, and is the reason why Transformers are so powerful.\n",
        "\n",
        "For simplicity, assume we have a single Head:\n",
        "\n",
        "1. The input has three parts extracted from it: the query $Q$, the key $K$, and the value $V$ (via projections or `Linear` layers).\n",
        "\n",
        "2. The query and key are multiplied together to get a score. This score is then scaled by the square root of the embedding dimension, $\\sqrt{d_k}$, then passed through a softmax to get the attention weights (*after* a masking operation is applied).\n",
        "\n",
        "3. The attention weights are then multiplied with the value to get the final output.\n",
        "\n",
        "When we extend this to *multiple heads*, we simply repeat this process for each head in parallel, and then concatenate the outputs of each head together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1U2Pn0d03fW",
        "outputId": "5f3aa26a-d193-4c5a-bc04-73152b6db531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 8, 256])\n",
            "torch.Size([32, 8, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MHSA(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.emb_dim = cfg.emb_dim\n",
        "        self.head_size = cfg.head_size\n",
        "        self.block_size = cfg.block_size\n",
        "        self.num_heads = cfg.num_heads\n",
        "\n",
        "        self.projection = nn.Linear(self.emb_dim, 3 * self.emb_dim)\n",
        "\n",
        "        self.out_projection = nn.Linear(self.emb_dim, self.emb_dim)\n",
        "\n",
        "        self.scale = 1.0 / (self.head_size ** 0.5)\n",
        "\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "      B, T, C = x.shape\n",
        "      H = self.num_heads\n",
        "\n",
        "      qkv = self.projection(x)\n",
        "\n",
        "      # print(qkv.shape)\n",
        "      query, key, value = torch.split(qkv, self.emb_dim, dim=-1)\n",
        "\n",
        "      query = query.view(B,T,H, self.head_size).transpose(1,2)\n",
        "      # print(\"The query shape is: \", query.shape)\n",
        "      key = key.view(B,T,H, self.head_size).transpose(1,2)\n",
        "      # print(\"The key shape is: \", key.shape)\n",
        "      value = value.view(B,T,H, self.head_size).transpose(1,2)\n",
        "      # print(\"The val shape is: \", value.shape)\n",
        "\n",
        "      attn_scores = torch.matmul(query, key.transpose(-1, -2)) * self.scale\n",
        "      # print(\"The shape of attn score is: \", attn_scores.shape)\n",
        "      attn_scores=attn_scores.masked_fill(self.mask[:T, :T]==0, float(\"-inf\"))\n",
        "      attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "      weighted_sum = torch.matmul(attn_probs, value)\n",
        "\n",
        "      weighted_sum = weighted_sum.transpose(1,2).reshape(B,T,-1)\n",
        "\n",
        "      # Apply the final projection\n",
        "      out = self.out_projection(weighted_sum)\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = torch.randn(32, config.block_size, config.emb_dim)\n",
        "print(x.shape)\n",
        "csa = MHSA(config)\n",
        "out = csa(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrafOEGs03fX"
      },
      "source": [
        "### Feedforward Network "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pbquKooA03fX"
      },
      "outputs": [],
      "source": [
        "class Feedforward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        emb_dim = config.emb_dim\n",
        "\n",
        "        self.linear1 = nn.Linear(emb_dim, emb_dim * 2)\n",
        "        self.linear2 = nn.Linear(emb_dim * 2, emb_dim * 2)\n",
        "        self.linear3 = nn.Linear(emb_dim * 2, emb_dim)\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.gelu = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.linear1(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C70wlVcS03fY"
      },
      "source": [
        "### Blocks with Skip Connections \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ryXX08G03fY",
        "outputId": "52c31909-91fb-4d8a-a0a7-f5912bfe41ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 8, 256])\n",
            "torch.Size([8, 8, 256])\n"
          ]
        }
      ],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mhsa = MHSA(config)\n",
        "        self.feedforward = Feedforward(config)\n",
        "        self.layer_norm1 = nn.LayerNorm(config.emb_dim)\n",
        "        self.layer_norm2 = nn.LayerNorm(config.emb_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual1 = x\n",
        "        x=self.layer_norm1(x)\n",
        "\n",
        "        x = self.mhsa(x)\n",
        "        x += residual1\n",
        "\n",
        "        residual2 = x\n",
        "\n",
        "        x=  self.layer_norm2(x)\n",
        "        x = self.feedforward(x)\n",
        "        x += residual2\n",
        "\n",
        "        return x\n",
        "\n",
        "x = torch.randn(8, config.block_size, config.emb_dim)\n",
        "print(x.shape)\n",
        "block = Block(config)\n",
        "out = block(x)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g7qcfwz03fY"
      },
      "source": [
        "### Putting it all together "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtBMyczc03fY",
        "outputId": "437a5b46-10d6-4523-eb9f-58dbccd427fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 27.364M\n"
          ]
        }
      ],
      "source": [
        "class SastaGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.block_size = config.block_size\n",
        "        self.num_layers = config.num_layers\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        # Word and position embeddings\n",
        "        self.word_embeddings = nn.Embedding(self.vocab_size, self.emb_dim)\n",
        "        self.position_embeddings = nn.Embedding(self.block_size, self.emb_dim)\n",
        "\n",
        "        # Sequence of Blocks\n",
        "        self.blocks = nn.ModuleList([Block(config) for _ in range(self.num_layers)])\n",
        "\n",
        "        # Final LayerNorm\n",
        "        self.final_layernorm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "        # Final linear layer (to get logits)\n",
        "        self.final_linear = nn.Linear(self.emb_dim, self.vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, idxs):\n",
        "\n",
        "        positions = torch.arange(idxs.size(1), device=idxs.device).expand(idxs.size()).long()\n",
        "        word_embeds = self.word_embeddings(idxs)\n",
        "        position_embeds = self.position_embeddings(positions)\n",
        "\n",
        "        embeddings = word_embeds + position_embeds\n",
        "\n",
        "        for block in self.blocks:\n",
        "            embeddings = block(embeddings)\n",
        "\n",
        "        # Apply the final LayerNorm\n",
        "        embeddings = self.final_layernorm(embeddings)\n",
        "\n",
        "        # Apply the final linear layer to get the logits\n",
        "        logits = self.final_linear(embeddings)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idxs, max_new_tokens=20):\n",
        "        '''\n",
        "        Takes in a sequence of indices (the tokenized sentence) and generates new tokens\n",
        "        Note that the input indices should not be longer than the block size\n",
        "        Returns the input sequence with the generated tokens appended (these should be decoded using the Tokenizer)\n",
        "\n",
        "        Params\n",
        "        ------\n",
        "        idxs: torch.Tensor\n",
        "            (B, T) tensor of token indices\n",
        "        max_new_tokens: int\n",
        "            Maximum number of new tokens to generate\n",
        "        '''\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idxs_trimmed = idxs[:, -self.block_size:]  # trim to block size\n",
        "\n",
        "            logits = self(idxs_trimmed)  # (B, T, V)\n",
        "\n",
        "            logits = logits[:, -1, :]  # (B, V)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, V)\n",
        "\n",
        "            next_idx = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "\n",
        "            idxs = torch.cat((idxs, next_idx), dim=1)  # (B, T+1)\n",
        "\n",
        "        return idxs\n",
        "\n",
        "cfg = Config()\n",
        "model = SastaGPT(cfg)\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.3f}M\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9cTfPwo03fZ",
        "outputId": "51c046c0-da87-4265-b036-620dc88095c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 8])\n",
            "torch.Size([32, 8, 50257])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-bc1ffde2c3f9>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x=torch.tensor(x, dtype=torch.long)\n",
            "<ipython-input-7-bc1ffde2c3f9>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y=torch.tensor(y, dtype=torch.long)\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch()\n",
        "print(xb.shape)\n",
        "\n",
        "logits = model(xb)\n",
        "print(logits.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bW1g5PJp03fa",
        "outputId": "55608439-9f07-46bb-aec4-7f03558776ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-bc1ffde2c3f9>:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x=torch.tensor(x, dtype=torch.long)\n",
            "<ipython-input-7-bc1ffde2c3f9>:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y=torch.tensor(y, dtype=torch.long)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [100/2000] , Loss: 1.9231761693954468\n",
            "Epoch [200/2000] , Loss: 1.8101130723953247\n",
            "Epoch [300/2000] , Loss: 2.201275110244751\n",
            "Epoch [400/2000] , Loss: 1.6508311033248901\n",
            "Epoch [500/2000] , Loss: 1.965787649154663\n",
            "Epoch [600/2000] , Loss: 1.9189720153808594\n",
            "Epoch [700/2000] , Loss: 2.2070119380950928\n",
            "Epoch [800/2000] , Loss: 1.8348371982574463\n",
            "Epoch [900/2000] , Loss: 1.4779247045516968\n",
            "Epoch [1000/2000] , Loss: 1.6749982833862305\n",
            "Epoch [1100/2000] , Loss: 1.4857405424118042\n",
            "Epoch [1200/2000] , Loss: 1.2660691738128662\n",
            "Epoch [1300/2000] , Loss: 1.3845367431640625\n",
            "Epoch [1400/2000] , Loss: 1.1707236766815186\n",
            "Epoch [1500/2000] , Loss: 1.431365966796875\n",
            "Epoch [1600/2000] , Loss: 0.5711421370506287\n",
            "Epoch [1700/2000] , Loss: 1.318962812423706\n",
            "Epoch [1800/2000] , Loss: 1.8411682844161987\n",
            "Epoch [1900/2000] , Loss: 1.1083104610443115\n",
            "Epoch [2000/2000] , Loss: 1.108543872833252\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "num_epochs = 2000\n",
        "\n",
        "total_iterations = 0\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    xb, yb = get_batch()\n",
        "\n",
        "    xb = xb.to(device)\n",
        "    yb = yb.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    preds = model(xb)\n",
        "\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), yb.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_iterations += 1\n",
        "\n",
        "    if total_iterations % 100 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] , Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIVt0-903fa",
        "outputId": "948de43d-bac5-43bb-80ab-b7d8cd722501"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Outer space suddenly on the black.\n",
            "                    BRAND\n",
            "      Brand looks over.\n",
            "                          CASE (CONT'D) (CONT'D)\n",
            "            Doyle looks at the bed, weapons\n",
            "            count them, with ourtech fires the inner doors of the and\n",
            "          'no gave with stars.\n",
            "         DOYLE\n",
            "           Cooper walks Pantagruel to happened to five farmer.\n",
            "                                  \n",
            "                               The rocket is no never important then pulls down of putting the\n",
            "            Sure it. EXT. olderAY, CHINESEOUTH on\n",
            "                    wormhole a military stretch of the bunker,. very iniously apart.\n",
            "                         \n",
            "                                  \n",
            "                      \n",
            "                   the window. PLAT'S FATHER\n",
            "                Cooper can arms. He\n",
            "                      INT. TRUCK -- DAY\n",
            "                   But you'd care.\n",
            "         \n",
            "                          Tars may\n",
            "                    tiny holes, then time. Don't\n",
            "                     \n",
            "                     \n",
            "                 \n",
            "           Cooper steps out of the shock pads to\n",
            "                                  \n",
            "         After a few hundred kilometers. If you\n",
            "               74.drivers\n",
            "              EXT. OBSER -- vanacan\n",
            "                                                  Frank flicker up with TIT the sky together,\n",
            "           Your wife for the wormhole of a 40,\n",
            "            gravitational TO: We should need to mission.\n",
            "                    \n",
            "             beginIGHT\n",
            "                       considerable Ill:\n",
            "           (points to DIA.\n",
            "                      Seconds before.\n",
            "          \n",
            "              \n",
            "                     BRAND\n",
            "           \n"
          ]
        }
      ],
      "source": [
        "sentence = \"Outer space\"\n",
        "idxs = torch.tensor(encoder.encode(sentence)).unsqueeze(0).to(device)\n",
        "\n",
        "model.eval()\n",
        "generated = model.generate(idxs, max_new_tokens=1000)\n",
        "res = encoder.decode(generated[0].cpu().numpy())\n",
        "print(res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOQWrn8I03fb"
      },
      "source": [
        "# Fin."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
